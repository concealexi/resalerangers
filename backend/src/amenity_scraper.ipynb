{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Community Clubs Scraper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "InvalidSessionIdException",
     "evalue": "Message: invalid session id: session deleted as the browser has closed the connection\nfrom disconnected: not connected to DevTools\n  (Session info: chrome=134.0.6998.178)\nStacktrace:\n\tGetHandleVerifier [0x00007FF733B04C25+3179557]\n\t(No symbol) [0x00007FF7337688A0]\n\t(No symbol) [0x00007FF7335F91CA]\n\t(No symbol) [0x00007FF7335E4F85]\n\t(No symbol) [0x00007FF733609F94]\n\t(No symbol) [0x00007FF73367F9DF]\n\t(No symbol) [0x00007FF73369FBE2]\n\t(No symbol) [0x00007FF733677A03]\n\t(No symbol) [0x00007FF7336406D0]\n\t(No symbol) [0x00007FF733641983]\n\tGetHandleVerifier [0x00007FF733B667CD+3579853]\n\tGetHandleVerifier [0x00007FF733B7D1D2+3672530]\n\tGetHandleVerifier [0x00007FF733B72153+3627347]\n\tGetHandleVerifier [0x00007FF7338D092A+868650]\n\t(No symbol) [0x00007FF733772FFF]\n\t(No symbol) [0x00007FF73376F4A4]\n\t(No symbol) [0x00007FF73376F646]\n\t(No symbol) [0x00007FF73375EAA9]\n\tBaseThreadInitThunk [0x00007FFE0AC8E8D7+23]\n\tRtlUserThreadStart [0x00007FFE0BB9BF6C+44]\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mInvalidSessionIdException\u001b[0m                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 93\u001b[0m\n\u001b[0;32m     90\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSaved results to \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcsv_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     92\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m---> 93\u001b[0m     \u001b[43mscrape_onepa\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[3], line 24\u001b[0m, in \u001b[0;36mscrape_onepa\u001b[1;34m()\u001b[0m\n\u001b[0;32m     20\u001b[0m all_results \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m     22\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m     23\u001b[0m     \u001b[38;5;66;03m# 1) Parse current page\u001b[39;00m\n\u001b[1;32m---> 24\u001b[0m     soup \u001b[38;5;241m=\u001b[39m BeautifulSoup(\u001b[43mdriver\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpage_source\u001b[49m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhtml.parser\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     25\u001b[0m     cc_divs \u001b[38;5;241m=\u001b[39m soup\u001b[38;5;241m.\u001b[39mfind_all(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdiv\u001b[39m\u001b[38;5;124m\"\u001b[39m, class_\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCCLocatorItem__inner__details\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     27\u001b[0m     \u001b[38;5;66;03m# Keep track of how many new items we add in this iteration\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\irvin\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\selenium\\webdriver\\remote\\webdriver.py:570\u001b[0m, in \u001b[0;36mWebDriver.page_source\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    562\u001b[0m \u001b[38;5;129m@property\u001b[39m\n\u001b[0;32m    563\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpage_source\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mstr\u001b[39m:\n\u001b[0;32m    564\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Gets the source of the current page.\u001b[39;00m\n\u001b[0;32m    565\u001b[0m \n\u001b[0;32m    566\u001b[0m \u001b[38;5;124;03m    Example:\u001b[39;00m\n\u001b[0;32m    567\u001b[0m \u001b[38;5;124;03m    --------\u001b[39;00m\n\u001b[0;32m    568\u001b[0m \u001b[38;5;124;03m    >>> print(driver.page_source)\u001b[39;00m\n\u001b[0;32m    569\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 570\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mCommand\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mGET_PAGE_SOURCE\u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvalue\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[1;32mc:\\Users\\irvin\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\selenium\\webdriver\\remote\\webdriver.py:429\u001b[0m, in \u001b[0;36mWebDriver.execute\u001b[1;34m(self, driver_command, params)\u001b[0m\n\u001b[0;32m    427\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_executor\u001b[38;5;241m.\u001b[39mexecute(driver_command, params)\n\u001b[0;32m    428\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m response:\n\u001b[1;32m--> 429\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43merror_handler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcheck_response\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresponse\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    430\u001b[0m     response[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvalue\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_unwrap_value(response\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvalue\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[0;32m    431\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m response\n",
      "File \u001b[1;32mc:\\Users\\irvin\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\selenium\\webdriver\\remote\\errorhandler.py:232\u001b[0m, in \u001b[0;36mErrorHandler.check_response\u001b[1;34m(self, response)\u001b[0m\n\u001b[0;32m    230\u001b[0m         alert_text \u001b[38;5;241m=\u001b[39m value[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124malert\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    231\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m exception_class(message, screen, stacktrace, alert_text)  \u001b[38;5;66;03m# type: ignore[call-arg]  # mypy is not smart enough here\u001b[39;00m\n\u001b[1;32m--> 232\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m exception_class(message, screen, stacktrace)\n",
      "\u001b[1;31mInvalidSessionIdException\u001b[0m: Message: invalid session id: session deleted as the browser has closed the connection\nfrom disconnected: not connected to DevTools\n  (Session info: chrome=134.0.6998.178)\nStacktrace:\n\tGetHandleVerifier [0x00007FF733B04C25+3179557]\n\t(No symbol) [0x00007FF7337688A0]\n\t(No symbol) [0x00007FF7335F91CA]\n\t(No symbol) [0x00007FF7335E4F85]\n\t(No symbol) [0x00007FF733609F94]\n\t(No symbol) [0x00007FF73367F9DF]\n\t(No symbol) [0x00007FF73369FBE2]\n\t(No symbol) [0x00007FF733677A03]\n\t(No symbol) [0x00007FF7336406D0]\n\t(No symbol) [0x00007FF733641983]\n\tGetHandleVerifier [0x00007FF733B667CD+3579853]\n\tGetHandleVerifier [0x00007FF733B7D1D2+3672530]\n\tGetHandleVerifier [0x00007FF733B72153+3627347]\n\tGetHandleVerifier [0x00007FF7338D092A+868650]\n\t(No symbol) [0x00007FF733772FFF]\n\t(No symbol) [0x00007FF73376F4A4]\n\t(No symbol) [0x00007FF73376F646]\n\t(No symbol) [0x00007FF73375EAA9]\n\tBaseThreadInitThunk [0x00007FFE0AC8E8D7+23]\n\tRtlUserThreadStart [0x00007FFE0BB9BF6C+44]\n"
     ]
    }
   ],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.common.exceptions import NoSuchElementException\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "import csv\n",
    "\n",
    "def scrape_onepa():\n",
    "    options = webdriver.ChromeOptions()\n",
    "    # options.add_argument(\"--headless\")  # run headless if desired\n",
    "    driver = webdriver.Chrome(options=options)\n",
    "\n",
    "    url = \"https://www.onepa.gov.sg/cc\"\n",
    "    driver.get(url)\n",
    "    time.sleep(5)  # let page load\n",
    "    \n",
    "    # We'll store *unique* results in a set (for easy deduping)\n",
    "    unique_set = set()\n",
    "    # Also keep a list of dicts for writing to CSV in the original order\n",
    "    all_results = []\n",
    "    \n",
    "    while True:\n",
    "        # 1) Parse current page\n",
    "        soup = BeautifulSoup(driver.page_source, \"html.parser\")\n",
    "        cc_divs = soup.find_all(\"div\", class_=\"CCLocatorItem__inner__details\")\n",
    "        \n",
    "        # Keep track of how many new items we add in this iteration\n",
    "        items_added_this_page = 0\n",
    "\n",
    "        for cc_div in cc_divs:\n",
    "            name_tag = cc_div.find(\"a\", class_=\"CCLocatorItem__inner__details--heading\")\n",
    "            name = name_tag.get_text(strip=True) if name_tag else \"\"\n",
    "            \n",
    "            p_tags = cc_div.find_all(\"p\", class_=\"CCLocatorItem__inner__details--label\")\n",
    "            address = p_tags[0].get_text(strip=True) if len(p_tags) > 0 else \"\"\n",
    "            tel = p_tags[1].get_text(strip=True) if len(p_tags) > 1 else \"\"\n",
    "            if tel.startswith(\"Tel:\"):\n",
    "                tel = tel.replace(\"Tel:\", \"\").strip()\n",
    "\n",
    "            # Create a tuple to represent uniqueness\n",
    "            item_tuple = (name, address, tel)\n",
    "\n",
    "            # Only add if not already seen\n",
    "            if item_tuple not in unique_set:\n",
    "                unique_set.add(item_tuple)\n",
    "                all_results.append({\n",
    "                    \"Name\": name,\n",
    "                    \"Address\": address,\n",
    "                    \"Tel\": tel\n",
    "                })\n",
    "                items_added_this_page += 1\n",
    "\n",
    "        # If we didn't add anything new on this page, it's time to stop\n",
    "        if items_added_this_page == 0:\n",
    "            print(\"No new results found on this page. Likely the last page or repeating data.\")\n",
    "            break\n",
    "\n",
    "        # 2) Try to click Next\n",
    "        try:\n",
    "            next_btn = driver.find_element(By.CSS_SELECTOR, \"span.btnNext\")\n",
    "            \n",
    "            # Scroll into view\n",
    "            driver.execute_script(\"arguments[0].scrollIntoView({block: 'center'});\", next_btn)\n",
    "            time.sleep(1)\n",
    "            \n",
    "            # JS click to avoid intercept issues\n",
    "            driver.execute_script(\"arguments[0].click();\", next_btn)\n",
    "            \n",
    "            # Wait for next page to load\n",
    "            time.sleep(3)\n",
    "        except NoSuchElementException:\n",
    "            print(\"No next button found. Probably the last page.\")\n",
    "            break\n",
    "        except Exception as e:\n",
    "            print(\"Error clicking Next:\", e)\n",
    "            break\n",
    "\n",
    "    driver.quit()\n",
    "\n",
    "    # 3) Save to CSV\n",
    "    csv_path = r\"C:\\Users\\irvin\\Downloads\\community_clubs_next_span.csv\" #change to appropriate filepath\n",
    "    with open(csv_path, \"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
    "        fieldnames = [\"Name\", \"Address\", \"Tel\"]\n",
    "        writer = csv.DictWriter(f, fieldnames=fieldnames)\n",
    "        writer.writeheader()\n",
    "        for row in all_results:\n",
    "            writer.writerow(row)\n",
    "    \n",
    "    print(f\"Scraped a total of {len(all_results)} **unique** items.\")\n",
    "    print(f\"Saved results to {csv_path}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    scrape_onepa()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Primary School Scraper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No new results found on this page. Stopping.\n",
      "Scraped a total of 182 unique items.\n",
      "Saved results to C:\\Users\\irvin\\Downloads\\schools.csv\n"
     ]
    }
   ],
   "source": [
    "def scrape_moe_schools():\n",
    "    options = webdriver.ChromeOptions()\n",
    "    # options.add_argument(\"--headless\")  # Uncomment to run headless if desired\n",
    "    driver = webdriver.Chrome(options=options)\n",
    "\n",
    "    # Replace this URL with the actual page you are scraping\n",
    "    url = \"https://www.moe.gov.sg/schoolfinder?journey=Primary%20school\"\n",
    "    driver.get(url)\n",
    "    time.sleep(5)  # Allow time for page to load fully\n",
    "\n",
    "    # Use a set for deduplication\n",
    "    unique_set = set()\n",
    "    all_results = []\n",
    "\n",
    "    while True:\n",
    "        # 1) Parse the current page with BeautifulSoup\n",
    "        soup = BeautifulSoup(driver.page_source, \"html.parser\")\n",
    "\n",
    "        # Example: each \"card\" or container might have the classes \"df fldr jcsb m-bxs aic\"\n",
    "        # Adjust if needed based on the actual structure in your HTML.\n",
    "        school_cards = soup.find_all(\"div\", class_=\"d:f fld:r jc:sb m-b:xs ai:c\")\n",
    "\n",
    "        items_added_this_page = 0\n",
    "\n",
    "        for card in school_cards:\n",
    "            # Grab the <p> with class \"ff:heading ts:l fw:6 c:grey-1 m-b:s\"\n",
    "            name_tag = card.find(\"p\", class_=\"ff:heading ts:l fw:6 c:grey-1 m-b:s\")\n",
    "            school_name = name_tag.get_text(strip=True) if name_tag else \"\"\n",
    "\n",
    "            # Grab the <p> with class \"ts:s c:grey-2 m-t:0 m-l:s p-l:m\"\n",
    "            address_tag = card.find(\"p\", class_=\"ts:s c:grey-2 m-t:0 m-l:s p-l:m\")\n",
    "            address = address_tag.get_text(strip=True) if address_tag else \"\"\n",
    "\n",
    "            # Create a tuple for deduping\n",
    "            item_tuple = (school_name, address)\n",
    "            if item_tuple not in unique_set:\n",
    "                unique_set.add(item_tuple)\n",
    "                all_results.append({\n",
    "                    \"SchoolName\": school_name,\n",
    "                    \"Address\": address\n",
    "                })\n",
    "                items_added_this_page += 1\n",
    "\n",
    "        # If no new items were added, we can assume we've reached the end\n",
    "        if items_added_this_page == 0:\n",
    "            print(\"No new results found on this page. Stopping.\")\n",
    "            break\n",
    "\n",
    "        # 2) Click the \"Next\" button to go to the next page\n",
    "        try:\n",
    "            # Based on your snapshot, this might work:\n",
    "            #   <button aria-label=\"next page\" class=\"mce-pagination__btn dir--right\">\n",
    "            #       <span class=\"icon-arrow-right\"></span>\n",
    "            #   </button>\n",
    "            # You can select by the button class or the span icon:\n",
    "            next_btn = driver.find_element(By.CSS_SELECTOR, \"button.moe-pagination__btn.dir--right\")\n",
    "\n",
    "            # Scroll into view (sometimes helpful)\n",
    "            driver.execute_script(\"arguments[0].scrollIntoView({block: 'center'});\", next_btn)\n",
    "            time.sleep(1)\n",
    "\n",
    "            # Click via JavaScript\n",
    "            driver.execute_script(\"arguments[0].click();\", next_btn)\n",
    "\n",
    "            # Wait a bit for the next page to load\n",
    "            time.sleep(3)\n",
    "\n",
    "        except NoSuchElementException:\n",
    "            print(\"No next button found. Probably the last page.\")\n",
    "            break\n",
    "        except Exception as e:\n",
    "            print(\"Error clicking Next:\", e)\n",
    "            break\n",
    "\n",
    "    # 3) Done scraping: close the browser\n",
    "    driver.quit()\n",
    "\n",
    "    # 4) Write results to CSV\n",
    "    csv_path = r\"C:\\Users\\irvin\\Downloads\\schools.csv\" #Change to appropriate file path\n",
    "    with open(csv_path, \"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
    "        fieldnames = [\"SchoolName\", \"Address\"]\n",
    "        writer = csv.DictWriter(f, fieldnames=fieldnames)\n",
    "        writer.writeheader()\n",
    "        for row in all_results:\n",
    "            writer.writerow(row)\n",
    "\n",
    "    print(f\"Scraped a total of {len(all_results)} unique items.\")\n",
    "    print(f\"Saved results to {csv_path}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    scrape_moe_schools()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initial PropertyGuru Scraper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'address': '658C Jurong West Street 65', 'listed_price': 'S$ 650,000', 'hdb_type': '5I HDB for sale', 'lease_year': '2000', 'sqft': '1,206 sqft'}\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import time\n",
    "from selenium import webdriver\n",
    "from bs4 import BeautifulSoup, Comment\n",
    "\n",
    "def property_guru(input_link):\n",
    "    options = webdriver.ChromeOptions()\n",
    "    # options.add_argument(\"--headless\")  # Uncomment to run headless if desired\n",
    "    driver = webdriver.Chrome(options=options)\n",
    "    driver.get(input_link)\n",
    "\n",
    "    # Give the page time to load\n",
    "    time.sleep(5)\n",
    "\n",
    "    # Parse the loaded page\n",
    "    soup = BeautifulSoup(driver.page_source, \"html.parser\")\n",
    "\n",
    "    # 1) Address\n",
    "    address_tag = soup.find(\"span\", class_=\"full-address__address\")\n",
    "    address = address_tag.get_text(strip=True) if address_tag else \"\"\n",
    "\n",
    "    # 2) Listed price\n",
    "    price_tag = soup.find(\"h2\", class_=\"amount\", attrs={\"data-automation-id\": \"overview-price-txt\"})\n",
    "    listed_price = price_tag.get_text(strip=True) if price_tag else \"\"\n",
    "\n",
    "    # 3) HDB type\n",
    "    hdb_type_tag = soup.find(\"div\", class_=\"meta-table__item__wrapper__value\", string=re.compile(\"HDB for sale\"))\n",
    "    hdb_type = hdb_type_tag.get_text(strip=True) if hdb_type_tag else \"\"\n",
    "\n",
    "    # 4) Lease year\n",
    "    lease_year_tag = soup.find(\"div\", class_=\"meta-table__item__wrapper__value\", string=re.compile(\"^TOP in\"))\n",
    "    lease_year = \"\"\n",
    "    if lease_year_tag:\n",
    "        text_val = lease_year_tag.get_text(strip=True)\n",
    "        match = re.search(r\"\\b\\d{4}\\b\", text_val)\n",
    "        if match:\n",
    "            lease_year = match.group(0)\n",
    "\n",
    "    # 5) Square feet extraction from <h4 class=\"amenity__text\">\n",
    "    amenity_tags = soup.find_all(\"h4\", class_=\"amenity__text\")\n",
    "    \n",
    "    sqft = \"\"\n",
    "    # Check if we have at least three such elements\n",
    "    if len(amenity_tags) >= 3:\n",
    "        # The third element (index 2) is where sqft is located\n",
    "        sqft_tag = amenity_tags[2]\n",
    "        \n",
    "        # Approach: extract text from the tag while ignoring comment nodes\n",
    "        lines = []\n",
    "        for element in sqft_tag.descendants:\n",
    "            if isinstance(element, Comment):\n",
    "                continue\n",
    "            if element.string and element.string.strip():\n",
    "                lines.append(element.string.strip())\n",
    "                \n",
    "        # Debug print to inspect what we got:\n",
    "        # print(\"Extracted lines:\", lines)\n",
    "        \n",
    "        # Expecting something like: ['1,109 == $0', 'sqft']\n",
    "        if len(lines) >= 2:\n",
    "            # Extract the number using regex from the first part\n",
    "            match = re.search(r'([\\d,]+)', lines[0])\n",
    "            if match and \"sqft\" in lines[1].lower():\n",
    "                sqft_number = match.group(1)\n",
    "                sqft = f\"{sqft_number} sqft\"\n",
    "        else:\n",
    "            # Fallback: try to extract using the entire text content\n",
    "            text_val = sqft_tag.get_text(separator=\" \", strip=True)\n",
    "            match = re.search(r'([\\d,]+).*?sqft', text_val, re.IGNORECASE)\n",
    "            if match:\n",
    "                sqft = f\"{match.group(1)} sqft\"\n",
    "\n",
    "    driver.quit()\n",
    "\n",
    "    data = {\n",
    "        \"address\": address,\n",
    "        \"listed_price\": listed_price,\n",
    "        \"hdb_type\": hdb_type,\n",
    "        \"lease_year\": lease_year,\n",
    "        \"sqft\": sqft\n",
    "    }\n",
    "    return data\n",
    "\n",
    "# Example usage:\n",
    "if __name__ == \"__main__\":\n",
    "    link = \"https://www.propertyguru.com.sg/listing/hdb-for-sale-658c-jurong-west-street-65-25503413\"\n",
    "    results = property_guru(link)\n",
    "    print(results)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
